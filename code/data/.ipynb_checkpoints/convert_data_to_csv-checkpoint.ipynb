{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "943d8fd8-8d8e-4fcf-a247-05f836e20e3e",
   "metadata": {},
   "source": [
    "# NOTES TO PUT IN REPORT\n",
    "\n",
    "To get the dataset, run\n",
    "_pip install parlai_ - Does not work with Python 3.10, I used Google Colab, but trying Python 3.9 or 3.8 could help.\n",
    "\n",
    "The files are located in \n",
    "_/usr/local/lib/python3.11/dist-packages/data_ - already separated into train, test, and valid, with some preprocessing files I may or may not use\n",
    "\n",
    "MAYBE I SHOULD USE CLASSIFICATION MODEL ON DATASET TOO (determine if a thought is even unhelpful in the first place. It would kinda suck to have a no-therapy tool that constantly reframes your already positive thoughts into different positive thoughts, making it hard to even tell if you know what you're doing or not)Score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b218bb2-f506-43f9-b266-34f4821ab5f3",
   "metadata": {},
   "source": [
    "**imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b580010-19cc-4cb3-affb-db007df6052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fabba801-b601-442f-bed7-1f94f4b16fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#precompile regex patterns for better performance (not redoing in each later function call)\n",
    "def precompile_mappings(mappings):\n",
    "    precompiled = {}\n",
    "    for key in mappings.keys():\n",
    "        #use word boundaries to avoid partial matches in words (\"I'm\" before \"I\")\n",
    "        #use escape key to ignore special characters\n",
    "        pattern = r\"\\b\" + re.escape(key) + r\"\\b\"\n",
    "        #turn pattern into regex object for later use (avoid recompiling regex in each check)\n",
    "        precompiled[key] = re.compile(pattern, flags=re.IGNORECASE)\n",
    "    return precompiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5cb03d-1a29-44bb-aa41-cb6166d0888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize all replacement text JSON files for data cleaning\n",
    "with open(f\"contractions_dict.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    contractions = json.load(file)\n",
    "with open(f\"slang_dict.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    slang = json.load(file)\n",
    "with open(f\"leftovers_dict.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    leftovers = json.load(file)\n",
    "\n",
    "#list for easy access and ordered replacements (all contractions, then all slang, then any leftovers)\n",
    "all_mappings = [contractions, slang, leftovers]\n",
    "\n",
    "\n",
    "#get dict of text with precompiled mappings (avoids recompiling every time they're found)\n",
    "precomp_contractions = precompile_mappings(contractions)\n",
    "precomp_slang = precompile_mappings(slang)\n",
    "precomp_leftovers = precompile_mappings(leftovers)\n",
    "\n",
    "precompiled_regex = [precomp_contractions, precomp_slang, precomp_leftovers]\n",
    "\n",
    "\n",
    "#get sorted keys for comparison (sort by longest -> shortest keys so \"I'm'a\"  gets checked before \"I'm\")\n",
    "#also saves time on resorting if done beforehand\n",
    "sorted_contractions = sorted(contractions.keys(), key=lambda x: -len(x))\n",
    "sorted_slang = sorted(slang.keys(), key=lambda x: -len(x))\n",
    "sorted_leftovers = sorted(leftovers.keys(), key=lambda x: -len(x))\n",
    "\n",
    "sorted_keys = [sorted_contractions, sorted_slang, sorted_leftovers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e27ded7-c5bf-44be-9b49-da4462b2fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run each replacement text mapping on given text (needed for all models, so I will do in this step) \n",
    "def normalize_text(text):\n",
    "    #run each replacement individually (one after another)\n",
    "    for (keys, patterns, mappings) in zip(sorted_keys, precompiled_regex, all_mappings):\n",
    "        for key in keys:\n",
    "            #uses precompiled regex pattern to replace with corresponding replacement text\n",
    "            text = patterns[key].sub(mappings[key], text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc86f79a-5bce-4d8b-ae77-1752ccbd83b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest input (with persona and pattern, without keywords): 108 words.\n",
      "Longest input (just negative_thought): 59 words.\n",
      "Longest output (reframed_thought): 70 words.\n",
      "train data done!\n",
      "\n",
      "Longest input (with persona and pattern, without keywords): 113 words.\n",
      "Longest input (just negative_thought): 56 words.\n",
      "Longest output (reframed_thought): 79 words.\n",
      "valid data done!\n",
      "\n",
      "Longest input (with persona and pattern, without keywords): 120 words.\n",
      "Longest input (just negative_thought): 45 words.\n",
      "Longest output (reframed_thought): 68 words.\n",
      "test data done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for iterating\n",
    "datasets = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "#for confirming sizes of datasets\n",
    "lengths = []\n",
    "\n",
    "#do process for each dataset\n",
    "for dataset in datasets:\n",
    "\n",
    "    #init\n",
    "    final_persona = []\n",
    "    final_pattern = []\n",
    "    final_negative_thoughts = []\n",
    "    final_reframed_thoughts = []\n",
    "\n",
    "    #get longest input/output (for knowing max token length in tokenizer)\n",
    "    longest_input = \"\"\n",
    "    longest_negative = \"\"\n",
    "    longest_reframe = \"\"\n",
    "    \n",
    "    #open line-delimited JSON file - each line is a separate JSON file to process separately\n",
    "    with open(f\"./reframe_thoughts_dataset/{dataset}.txt\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            #original text files has \\u2019 (on top of \\u0027 for some reason), the former of which messed with data\n",
    "            #couldn't find way to fix it that worked, so I ran this on copy of dataset with them removed\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            #normalize each input to remove contractions/slang/etc.\n",
    "            persona = normalize_text(obj[\"persona\"])\n",
    "            pattern = normalize_text(obj[\"pattern\"])\n",
    "            thought = normalize_text(obj[\"thought\"])\n",
    "\n",
    "            #check if negative thought is longest - reassign if it is\n",
    "            if len(thought.split()) >= len(longest_negative.split()):\n",
    "                longest_negative = thought\n",
    "            \n",
    "            #get current input\n",
    "            long_input = persona + \" \" + pattern + \" \" + thought\n",
    "            #see if longer than longest recorded          \n",
    "            if len(long_input.split()) >= len(longest_input.split()):\n",
    "                longest_input = long_input\n",
    "            \n",
    "            #duplicate other inputs based on number of reframed thoughts (creates perfect input-output pairs)\n",
    "            #this does increase chance of overfitting, so hyperparams will be tuned to make sure that doesn't happen\n",
    "            for reframe in obj[\"reframes\"]:\n",
    "\n",
    "                #do same for output\n",
    "                reframe_thought = normalize_text(reframe[\"reframe\"])\n",
    "\n",
    "                #see if longest output\n",
    "                if len(reframe_thought.split()) >= len(longest_reframe.split()):\n",
    "                    longest_reframe = reframe_thought\n",
    "                \n",
    "                #add to appropriate lists\n",
    "                final_persona.append(f\"{persona}\\n\")\n",
    "                final_pattern.append(f\"{pattern}\\n\")\n",
    "                final_negative_thoughts.append(f\"{thought}\\n\")\n",
    "                final_reframed_thoughts.append(f\"{reframe_thought}\\n\")\n",
    "\n",
    "    #add into dataframes for csv storage\n",
    "    required_data = pd.DataFrame({\n",
    "        \"negative_thought\": final_negative_thoughts,\n",
    "        \"reframed_thought\": final_reframed_thoughts\n",
    "    })\n",
    "    all_data = pd.DataFrame({\n",
    "        \"persona\": final_persona,\n",
    "        \"pattern\": final_pattern,\n",
    "        \"negative_thought\": final_negative_thoughts,\n",
    "        \"reframed_thought\": final_reframed_thoughts\n",
    "    })\n",
    "\n",
    "    #add length to lengths for vis of data split - any of the lists work for this\n",
    "    lengths.append(len(final_reframed_thoughts))\n",
    "\n",
    "    #save new datasets\n",
    "    required_data.to_csv(f\"{dataset}_data.csv\", index=False, encoding=\"utf-8\")\n",
    "    all_data.to_csv(f\"all_{dataset}_data.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    #see longest input and output\n",
    "    print(f\"Longest input (with persona and pattern, without keywords): {len(longest_input.split())} words.\")\n",
    "    print(f\"Longest input (just negative_thought): {len(longest_negative.split())} words.\")\n",
    "    print(f\"Longest output (reframed_thought): {len(longest_reframe.split())} words.\")\n",
    "    \n",
    "    print(f\"{dataset} data done!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd2da0c2-a624-4029-aea7-64c93de25478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset Size: 26507\n",
      "\n",
      "Training Data Size: 18635 samples - 70.30% of data\n",
      "Validation Data Size: 5249 samples - 19.80% of data\n",
      "Testing Data Size: 2623 samples - 9.90% of data\n"
     ]
    }
   ],
   "source": [
    "total_length = lengths[0] + lengths[1] + lengths[2]\n",
    "\n",
    "print(f\"Total Dataset Size: {total_length}\\n\")\n",
    "print(f\"Training Data Size: {lengths[0]} samples - {(lengths[0] / total_length * 100):.2f}% of data\")\n",
    "print(f\"Validation Data Size: {lengths[1]} samples - {(lengths[1] / total_length * 100):.2f}% of data\")\n",
    "print(f\"Testing Data Size: {lengths[2]} samples - {(lengths[2] / total_length * 100):.2f}% of data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
