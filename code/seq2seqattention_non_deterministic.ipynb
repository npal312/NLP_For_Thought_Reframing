{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02e7b8ab",
   "metadata": {
    "papermill": {
     "duration": 0.004654,
     "end_time": "2025-05-05T07:02:07.341907",
     "exception": false,
     "start_time": "2025-05-05T07:02:07.337253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f5a149",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:07.350870Z",
     "iopub.status.busy": "2025-05-05T07:02:07.350524Z",
     "iopub.status.idle": "2025-05-05T07:02:15.153217Z",
     "shell.execute_reply": "2025-05-05T07:02:15.152445Z"
    },
    "papermill": {
     "duration": 7.808948,
     "end_time": "2025-05-05T07:02:15.154756",
     "exception": false,
     "start_time": "2025-05-05T07:02:07.345808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92506d",
   "metadata": {
    "papermill": {
     "duration": 0.003618,
     "end_time": "2025-05-05T07:02:15.162466",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.158848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing/Model Preparations/Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae3618c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.170748Z",
     "iopub.status.busy": "2025-05-05T07:02:15.170403Z",
     "iopub.status.idle": "2025-05-05T07:02:15.301722Z",
     "shell.execute_reply": "2025-05-05T07:02:15.300862Z"
    },
    "papermill": {
     "duration": 0.136829,
     "end_time": "2025-05-05T07:02:15.302864",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.166035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download Punkt tokenizer data - necessary for sentence tokenization, which word_tokenize needs\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d1ba2a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.311880Z",
     "iopub.status.busy": "2025-05-05T07:02:15.311443Z",
     "iopub.status.idle": "2025-05-05T07:02:15.315807Z",
     "shell.execute_reply": "2025-05-05T07:02:15.315066Z"
    },
    "papermill": {
     "duration": 0.009936,
     "end_time": "2025-05-05T07:02:15.316957",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.307021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define hyperparams\n",
    "BATCH_SIZE = 64 #good batch size for this task (raised to use more GPU and less CPU)\n",
    "EMBED_SIZE = 256 #large enough for capturing word relationships but not too much to overfit\n",
    "HIDDEN_SIZE = 512 #same here\n",
    "EPOCHS = 20 #high number of epochs to ensure model trains well, will implement early stopping to counteract\n",
    "PATIENCE = 3 #amount of epochs to wait for until early stop - good val, considering total epoch num\n",
    "MAX_LEN = 60 #fits all data in dataset without truncating\n",
    "PAD_VAL = 2 #for designating what id pad_token is (used in pad batch and cross entropy loss) #maybe make when vocab made\n",
    "DROPOUT_RATE = 0.5 #to avoid overfitting\n",
    "LEARNING_RATE = 0.0001 #10x below initial Adam optimizer learning rate (initial test showed valid loss stagnate by epoch 4, hopefully this makes it better)\n",
    "#teacher forcing - gives true prev token as input to model at specified timesteps to help with learning and avoid errors chaining\n",
    "TEACHER_FORCING_RATIO = 0.5 #50% chance true token will be used vs model's prev prediction (so not always learning from either one)\n",
    "RANDOM = 213 #random state for reproducibility (I like using this num)\n",
    "TEMPERATURE = 0.8 #controls randomness (higher = more random but less accurate/coherent - 0.8 is balanced choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb75b24f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.325839Z",
     "iopub.status.busy": "2025-05-05T07:02:15.325276Z",
     "iopub.status.idle": "2025-05-05T07:02:15.329129Z",
     "shell.execute_reply": "2025-05-05T07:02:15.328449Z"
    },
    "papermill": {
     "duration": 0.009239,
     "end_time": "2025-05-05T07:02:15.330132",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.320893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#randomness\n",
    "random.seed(RANDOM)\n",
    "np.random.seed(RANDOM)\n",
    "torch.backends.cudnn.deterministic = True #ensures GPU operations are deterministic (only for training)\n",
    "torch.backends.cudnn.benchmark = False #disables auto-tuning of algorithms (ensures more reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d493f52f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.338561Z",
     "iopub.status.busy": "2025-05-05T07:02:15.338350Z",
     "iopub.status.idle": "2025-05-05T07:02:15.400048Z",
     "shell.execute_reply": "2025-05-05T07:02:15.399352Z"
    },
    "papermill": {
     "duration": 0.067168,
     "end_time": "2025-05-05T07:02:15.401140",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.333972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#make directory to save final model\n",
    "final_dir = \"./final/seq2seq_non\"\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "#make directory to save final data\n",
    "model_outputs_dir = \"./model_outputs\"\n",
    "os.makedirs(model_outputs_dir, exist_ok=True)\n",
    "\n",
    "#init special tokens - no mask tokens needed\n",
    "sos_token = \"<s>\"\n",
    "eos_token = \"</s>\"\n",
    "pad_token = \"<pad>\"\n",
    "unk_token = \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82cfc55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.409850Z",
     "iopub.status.busy": "2025-05-05T07:02:15.409626Z",
     "iopub.status.idle": "2025-05-05T07:02:15.412865Z",
     "shell.execute_reply": "2025-05-05T07:02:15.412306Z"
    },
    "papermill": {
     "duration": 0.008627,
     "end_time": "2025-05-05T07:02:15.413872",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.405245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenize words w/ word_tokenize from nltk (handles punctuation & numbers properly (as separate tokens, good for natural text gen))\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text.lower()) #bc of above, only making lowercase is needed (contractions were removed, so won't be an issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42cfb3d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.422529Z",
     "iopub.status.busy": "2025-05-05T07:02:15.422292Z",
     "iopub.status.idle": "2025-05-05T07:02:15.426356Z",
     "shell.execute_reply": "2025-05-05T07:02:15.425669Z"
    },
    "papermill": {
     "duration": 0.0098,
     "end_time": "2025-05-05T07:02:15.427488",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.417688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#build vocab with every word encountered so no loss in generation ability (unknown tokens would make it difficult)\n",
    "def build_vocab(sentences):\n",
    "    #init with special tokens\n",
    "    vocab = {sos_token: 0, eos_token: 1, pad_token: 2, unk_token: 3}\n",
    "    \n",
    "    #iterate through all tokenized sentences\n",
    "    for sent in sentences:\n",
    "        for word in tokenize(sent):\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab) #add to vocab with new id\n",
    "                \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b329756a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.435717Z",
     "iopub.status.busy": "2025-05-05T07:02:15.435508Z",
     "iopub.status.idle": "2025-05-05T07:02:15.439056Z",
     "shell.execute_reply": "2025-05-05T07:02:15.438363Z"
    },
    "papermill": {
     "duration": 0.008922,
     "end_time": "2025-05-05T07:02:15.440190",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.431268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#assign tokens to ids in sentences using vocab\n",
    "def token_to_ids(sentence, vocab):\n",
    "    #replace with <unk> if not in vocab, else id\n",
    "    return [vocab.get(word, vocab[unk_token]) for word in tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f1d20e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.448518Z",
     "iopub.status.busy": "2025-05-05T07:02:15.448279Z",
     "iopub.status.idle": "2025-05-05T07:02:15.453599Z",
     "shell.execute_reply": "2025-05-05T07:02:15.452905Z"
    },
    "papermill": {
     "duration": 0.010696,
     "end_time": "2025-05-05T07:02:15.454643",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.443947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#put data in custom dataset so it works with DataLoader (won't work with anything without __len__ and __getitem__)\n",
    "class ThoughtDataset(Dataset):\n",
    "    def __init__(self, df, src_vocab, tgt_vocab):\n",
    "\n",
    "        self.df = df.reset_index(drop=False) #stores original row indices\n",
    "        #make input_output pairs\n",
    "        self.pairs = list(zip(df[\"negative_thought\"], df[\"reframed_thought\"]))\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.pairs[idx]\n",
    "        index = self.df.loc[idx, \"index\"] #get preserved index\n",
    "        \n",
    "        #get tokens\n",
    "        src_tokens = token_to_ids(src, self.src_vocab)\n",
    "        tgt_tokens = [self.tgt_vocab[sos_token]] + token_to_ids(tgt, self.tgt_vocab) + [self.tgt_vocab[eos_token]]\n",
    "        \n",
    "        #make into pytorch tensors, send to GPU\n",
    "        return torch.tensor(src_tokens).to(device), torch.tensor(tgt_tokens).to(device), index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf49ef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.462686Z",
     "iopub.status.busy": "2025-05-05T07:02:15.462483Z",
     "iopub.status.idle": "2025-05-05T07:02:15.466174Z",
     "shell.execute_reply": "2025-05-05T07:02:15.465538Z"
    },
    "papermill": {
     "duration": 0.008856,
     "end_time": "2025-05-05T07:02:15.467169",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.458313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pad batches to make same size (use as collate function)\n",
    "def pad_batch(batch):\n",
    "    src_batch, tgt_batch, indices_batch = zip(*batch)\n",
    "    \n",
    "    #use pad sequence - finds longest sequence in batch to automatically pad to\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_VAL, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_VAL, batch_first=True)\n",
    "    \n",
    "    return src_batch.to(device), tgt_batch.to(device), indices_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92115ed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.475471Z",
     "iopub.status.busy": "2025-05-05T07:02:15.475255Z",
     "iopub.status.idle": "2025-05-05T07:02:15.478779Z",
     "shell.execute_reply": "2025-05-05T07:02:15.478106Z"
    },
    "papermill": {
     "duration": 0.008922,
     "end_time": "2025-05-05T07:02:15.479849",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.470927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get non-deterministic output\n",
    "def sample_next_token(prob_dist, temperature=1.0):\n",
    "    prob_dist = torch.softmax(prob_dist / temperature, dim=1)  #apply temp\n",
    "    pred_token = torch.multinomial(prob_dist, 1)  #sample from distribution\n",
    "    \n",
    "    return pred_token.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0d74465",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.488011Z",
     "iopub.status.busy": "2025-05-05T07:02:15.487812Z",
     "iopub.status.idle": "2025-05-05T07:02:15.492100Z",
     "shell.execute_reply": "2025-05-05T07:02:15.491455Z"
    },
    "papermill": {
     "duration": 0.009542,
     "end_time": "2025-05-05T07:02:15.493095",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.483553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#encoder class (pytorch)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=PAD_VAL)\n",
    "        \n",
    "        #gru time\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    \n",
    "    #call function (uses forward for pytorch methods)\n",
    "    def forward(self, src):\n",
    "        \n",
    "        #get embeddings\n",
    "        embed = self.embedding(src)\n",
    "        \n",
    "        #do gru\n",
    "        enc_outputs, hidden_state = self.gru(embed)\n",
    "        \n",
    "        return enc_outputs, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b268e266",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.501982Z",
     "iopub.status.busy": "2025-05-05T07:02:15.501390Z",
     "iopub.status.idle": "2025-05-05T07:02:15.506457Z",
     "shell.execute_reply": "2025-05-05T07:02:15.505755Z"
    },
    "papermill": {
     "duration": 0.010665,
     "end_time": "2025-05-05T07:02:15.507500",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.496835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#attention - specifically additive (or bahdanau) attention class (pytorch)\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        #attention linear layer - concatenates decoder hidden state and enc_output\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        #linear layer that outputs scalar - computes score (or \"energy\") for each position in enc_output's sequence\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    \n",
    "    def forward(self, hidden_state, enc_outputs):\n",
    "\n",
    "        #init\n",
    "        batch_size = enc_outputs.size(0)\n",
    "        src_len = enc_outputs.size(1)\n",
    "\n",
    "        #repeating hidden state for attention calc (add extra dim for score calc)\n",
    "        hidden_state = hidden_state.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        #score time (non-linear transformat to attention layer, which gets hidden state and enc_output concat)\n",
    "        score = torch.tanh(self.attn(torch.cat((hidden_state, enc_outputs), dim=2)))\n",
    "        \n",
    "        #get attention score (scalar) for each position in src sequence\n",
    "        attention = self.v(score).squeeze(2)\n",
    "        \n",
    "        #get attention weights - prob dist from softmax\n",
    "        attn_weights = torch.softmax(attention, dim=1)\n",
    "        \n",
    "        return attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93efced6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.515579Z",
     "iopub.status.busy": "2025-05-05T07:02:15.515356Z",
     "iopub.status.idle": "2025-05-05T07:02:15.521400Z",
     "shell.execute_reply": "2025-05-05T07:02:15.520718Z"
    },
    "papermill": {
     "duration": 0.011318,
     "end_time": "2025-05-05T07:02:15.522476",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.511158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#decoder class (pytorch)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        #init\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        #embedding\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim, padding_idx=PAD_VAL)\n",
    "        \n",
    "        #gru\n",
    "        self.gru = nn.GRU(embedding_dim + hidden_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        #dropout\n",
    "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
    "        \n",
    "        #fully connected output layer \n",
    "        self.out_layer = nn.Linear(embedding_dim + hidden_dim * 2, output_dim)\n",
    "\n",
    "    \n",
    "    def forward(self, tgt_input, hidden_state, enc_outputs):\n",
    "        #add dim to input - ONLY if tgt_input is 1D beforehand\n",
    "        if tgt_input.dim() == 1:\n",
    "            tgt_input = tgt_input.unsqueeze(1)\n",
    "        \n",
    "        #embed\n",
    "        embedded = self.embedding(tgt_input.to(device))\n",
    "        \n",
    "        #dropping out\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        #get attention weights - which parts of enc_output to focus on\n",
    "        attn_weights = self.attention(hidden_state[-1], enc_outputs).unsqueeze(1)\n",
    "        \n",
    "        #get context vector - shows decoder which parts to focus on\n",
    "        context = torch.bmm(attn_weights, enc_outputs)\n",
    "        \n",
    "        #get input for GRU\n",
    "        gru_input = torch.cat((embedded, context), dim=2)\n",
    "        \n",
    "        #gru time\n",
    "        output, hidden_state = self.gru(gru_input, hidden_state)\n",
    "        \n",
    "        #get output of output layer (logits for each word in vocab at current time step)\n",
    "        dec_outputs = self.out_layer(torch.cat((output.squeeze(1), context.squeeze(1), embedded.squeeze(1)), dim=1))\n",
    "        \n",
    "        return dec_outputs, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12f6ac50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.530713Z",
     "iopub.status.busy": "2025-05-05T07:02:15.530512Z",
     "iopub.status.idle": "2025-05-05T07:02:15.535783Z",
     "shell.execute_reply": "2025-05-05T07:02:15.535268Z"
    },
    "papermill": {
     "duration": 0.010575,
     "end_time": "2025-05-05T07:02:15.536796",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.526221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#seq2seq class (pytorch)\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        #init encoder and decoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        #init\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(device)\n",
    "\n",
    "        #do encoder\n",
    "        enc_outputs, hidden_state = self.encoder(src)\n",
    "        \n",
    "        #get tgt input\n",
    "        tgt_input = tgt[:, 0].to(device)\n",
    "        \n",
    "        #do decoder\n",
    "        for t in range(1, tgt_len):\n",
    "            #get decoder output & store it\n",
    "            dec_output, hidden_state = self.decoder(tgt_input, hidden_state, enc_outputs)\n",
    "            outputs[:, t] = dec_output\n",
    "            \n",
    "            #decide whether teacher forces or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #sample token from predicted prob distribution (not just top token)\n",
    "            prob_dist = torch.softmax(dec_output, dim=1)\n",
    "            pred_token = sample_next_token(prob_dist, temperature=TEMPERATURE)\n",
    "            \n",
    "            #use either true target token or model predicted token as input\n",
    "            tgt_input = tgt[:, t].to(device) if teacher_force else pred_token.to(device)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6f576e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:15.545010Z",
     "iopub.status.busy": "2025-05-05T07:02:15.544819Z",
     "iopub.status.idle": "2025-05-05T07:02:19.858252Z",
     "shell.execute_reply": "2025-05-05T07:02:19.857500Z"
    },
    "papermill": {
     "duration": 4.319162,
     "end_time": "2025-05-05T07:02:19.859728",
     "exception": false,
     "start_time": "2025-05-05T07:02:15.540566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load raw dataset\n",
    "train_df = pd.read_csv(\"./data/train_data.csv\")\n",
    "valid_df = pd.read_csv(\"./data/valid_data.csv\")\n",
    "\n",
    "#build vocab off of training data - including inverse_vocab for target (for making readable at end)\n",
    "src_vocab = build_vocab(train_df[\"negative_thought\"])\n",
    "tgt_vocab = build_vocab(train_df[\"reframed_thought\"])\n",
    "inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "#wrap in dataset\n",
    "train_dataset = ThoughtDataset(train_df, src_vocab, tgt_vocab)\n",
    "valid_dataset = ThoughtDataset(valid_df, src_vocab, tgt_vocab)\n",
    "\n",
    "#use DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "637c8d07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:19.870274Z",
     "iopub.status.busy": "2025-05-05T07:02:19.869847Z",
     "iopub.status.idle": "2025-05-05T07:02:24.797931Z",
     "shell.execute_reply": "2025-05-05T07:02:24.797379Z"
    },
    "papermill": {
     "duration": 4.934302,
     "end_time": "2025-05-05T07:02:24.799231",
     "exception": false,
     "start_time": "2025-05-05T07:02:19.864929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#init model - encoder, attention mechanism, decoder, and entire seq2seq model\n",
    "encoder = Encoder(len(src_vocab), EMBED_SIZE, HIDDEN_SIZE)\n",
    "attention = Attention(HIDDEN_SIZE)\n",
    "decoder = Decoder(len(tgt_vocab), EMBED_SIZE, HIDDEN_SIZE, attention)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "#get optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE) #adam optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_VAL) #ignore padding tokens for loss calc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e0b179",
   "metadata": {
    "papermill": {
     "duration": 0.003594,
     "end_time": "2025-05-05T07:02:24.806889",
     "exception": false,
     "start_time": "2025-05-05T07:02:24.803295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5707d540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:02:24.815190Z",
     "iopub.status.busy": "2025-05-05T07:02:24.814878Z",
     "iopub.status.idle": "2025-05-05T07:37:22.528897Z",
     "shell.execute_reply": "2025-05-05T07:37:22.528048Z"
    },
    "papermill": {
     "duration": 2097.719927,
     "end_time": "2025-05-05T07:37:22.530469",
     "exception": false,
     "start_time": "2025-05-05T07:02:24.810542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 5.9028\n",
      "Epoch 1, Validation Loss: 5.6502\n",
      "Epoch 1 summary:\n",
      "  Training time: 91.86 seconds\n",
      "  Validation time: 13.84 seconds\n",
      "  Total epoch time: 105.70 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 2, Training Loss: 5.2351\n",
      "Epoch 2, Validation Loss: 5.5970\n",
      "Epoch 2 summary:\n",
      "  Training time: 90.68 seconds\n",
      "  Validation time: 13.93 seconds\n",
      "  Total epoch time: 104.61 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 3, Training Loss: 5.0469\n",
      "Epoch 3, Validation Loss: 5.5594\n",
      "Epoch 3 summary:\n",
      "  Training time: 90.81 seconds\n",
      "  Validation time: 13.80 seconds\n",
      "  Total epoch time: 104.61 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 4, Training Loss: 4.8958\n",
      "Epoch 4, Validation Loss: 5.5014\n",
      "Epoch 4 summary:\n",
      "  Training time: 90.32 seconds\n",
      "  Validation time: 13.86 seconds\n",
      "  Total epoch time: 104.18 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 5, Training Loss: 4.7622\n",
      "Epoch 5, Validation Loss: 5.4194\n",
      "Epoch 5 summary:\n",
      "  Training time: 90.25 seconds\n",
      "  Validation time: 13.94 seconds\n",
      "  Total epoch time: 104.19 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 6, Training Loss: 4.6231\n",
      "Epoch 6, Validation Loss: 5.3340\n",
      "Epoch 6 summary:\n",
      "  Training time: 89.00 seconds\n",
      "  Validation time: 13.85 seconds\n",
      "  Total epoch time: 102.85 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 7, Training Loss: 4.5082\n",
      "Epoch 7, Validation Loss: 5.2368\n",
      "Epoch 7 summary:\n",
      "  Training time: 90.46 seconds\n",
      "  Validation time: 13.84 seconds\n",
      "  Total epoch time: 104.30 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 8, Training Loss: 4.3548\n",
      "Epoch 8, Validation Loss: 5.1194\n",
      "Epoch 8 summary:\n",
      "  Training time: 90.48 seconds\n",
      "  Validation time: 13.94 seconds\n",
      "  Total epoch time: 104.42 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 9, Training Loss: 4.2252\n",
      "Epoch 9, Validation Loss: 5.0421\n",
      "Epoch 9 summary:\n",
      "  Training time: 90.08 seconds\n",
      "  Validation time: 14.06 seconds\n",
      "  Total epoch time: 104.14 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 10, Training Loss: 4.1016\n",
      "Epoch 10, Validation Loss: 4.9720\n",
      "Epoch 10 summary:\n",
      "  Training time: 90.33 seconds\n",
      "  Validation time: 13.83 seconds\n",
      "  Total epoch time: 104.16 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 11, Training Loss: 3.9990\n",
      "Epoch 11, Validation Loss: 4.9042\n",
      "Epoch 11 summary:\n",
      "  Training time: 90.70 seconds\n",
      "  Validation time: 14.02 seconds\n",
      "  Total epoch time: 104.73 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 12, Training Loss: 3.8944\n",
      "Epoch 12, Validation Loss: 4.8534\n",
      "Epoch 12 summary:\n",
      "  Training time: 89.80 seconds\n",
      "  Validation time: 13.92 seconds\n",
      "  Total epoch time: 103.73 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 13, Training Loss: 3.8161\n",
      "Epoch 13, Validation Loss: 4.8278\n",
      "Epoch 13 summary:\n",
      "  Training time: 90.22 seconds\n",
      "  Validation time: 13.88 seconds\n",
      "  Total epoch time: 104.09 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 14, Training Loss: 3.7439\n",
      "Epoch 14, Validation Loss: 4.7763\n",
      "Epoch 14 summary:\n",
      "  Training time: 90.91 seconds\n",
      "  Validation time: 13.97 seconds\n",
      "  Total epoch time: 104.88 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 15, Training Loss: 3.6663\n",
      "Epoch 15, Validation Loss: 4.7840\n",
      "Epoch 15 summary:\n",
      "  Training time: 89.78 seconds\n",
      "  Validation time: 13.93 seconds\n",
      "  Total epoch time: 103.71 seconds\n",
      "\n",
      "No Validation Loss Improvement. Num of epochs with no improvement: 1/3.\n",
      "Epoch 16, Training Loss: 3.6100\n",
      "Epoch 16, Validation Loss: 4.7603\n",
      "Epoch 16 summary:\n",
      "  Training time: 90.64 seconds\n",
      "  Validation time: 13.79 seconds\n",
      "  Total epoch time: 104.43 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 17, Training Loss: 3.5473\n",
      "Epoch 17, Validation Loss: 4.7271\n",
      "Epoch 17 summary:\n",
      "  Training time: 90.46 seconds\n",
      "  Validation time: 13.78 seconds\n",
      "  Total epoch time: 104.23 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 18, Training Loss: 3.5328\n",
      "Epoch 18, Validation Loss: 4.7038\n",
      "Epoch 18 summary:\n",
      "  Training time: 90.14 seconds\n",
      "  Validation time: 14.05 seconds\n",
      "  Total epoch time: 104.19 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 19, Training Loss: 3.4459\n",
      "Epoch 19, Validation Loss: 4.6996\n",
      "Epoch 19 summary:\n",
      "  Training time: 90.40 seconds\n",
      "  Validation time: 13.82 seconds\n",
      "  Total epoch time: 104.23 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 20, Training Loss: 3.4054\n",
      "Epoch 20, Validation Loss: 4.6872\n",
      "Epoch 20 summary:\n",
      "  Training time: 90.59 seconds\n",
      "  Validation time: 13.81 seconds\n",
      "  Total epoch time: 104.40 seconds\n",
      "\n",
      "Validation Loss improved!\n"
     ]
    }
   ],
   "source": [
    "#for early stop\n",
    "best_valid_loss = float(\"inf\") #init with high valid loss\n",
    "stagnating_epochs = 0 #epoch num with no improvement\n",
    "\n",
    "#training\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time() #how long whole epoch takes\n",
    "    train_start = time.time() #how long training takes\n",
    "    model.train() #training mode\n",
    "    train_loss = 0 #init loss\n",
    "    \n",
    "    for src, tgt, _ in train_loader:\n",
    "        optimizer.zero_grad() #clear optimized tensor gradients that accumulate in backward pass\n",
    "        \n",
    "        output = model(src, tgt, TEACHER_FORCING_RATIO) #get output token\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        tgt = tgt[:, 1:].reshape(-1) #get true token\n",
    "        \n",
    "        loss = criterion(output, tgt) #compare (loss time)\n",
    "        loss.backward() #backpropagation!!!\n",
    "        optimizer.step() #update weights\n",
    "        train_loss += loss.item() #add to loss\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss/len(train_loader):.4f}\")\n",
    "    train_end = time.time()\n",
    "\n",
    "    #validation loop fr\n",
    "    valid_start = time.time() #how long validation takes\n",
    "    model.eval() #bc we're predicting rn\n",
    "    valid_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, _ in valid_loader:\n",
    "            #same thing but no teacher forcing and no backprop/updating weights\n",
    "            output = model(src, tgt, 0)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            tgt = tgt[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt)\n",
    "            valid_loss += loss.item() #add to loss\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {valid_loss/len(valid_loader):.4f}\")\n",
    "    valid_end = time.time()\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} summary:\")\n",
    "    print(f\"  Training time: {train_end - train_start:.2f} seconds\")\n",
    "    print(f\"  Validation time: {valid_end - valid_start:.2f} seconds\")\n",
    "    print(f\"  Total epoch time: {epoch_end - epoch_start:.2f} seconds\\n\")\n",
    "\n",
    "    #early stop check! (if better valid loss, update. otherwise, update stagnating epochs) \n",
    "    if valid_loss/len(valid_loader) < best_valid_loss:\n",
    "        best_valid_loss = valid_loss/len(valid_loader) \n",
    "        stagnating_epochs = 0 #reset\n",
    "        print(f\"Validation Loss improved!\")\n",
    "        #save model with best validation loss (in case of early stop)\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": loss.item()\n",
    "        }, os.path.join(final_dir, \"best_model.pth\"))\n",
    "    else:\n",
    "        stagnating_epochs += 1 #increase\n",
    "        print(f\"No Validation Loss Improvement. Num of epochs with no improvement: {stagnating_epochs}/{PATIENCE}.\")\n",
    "\n",
    "    #if patience reached, end\n",
    "    if stagnating_epochs >= PATIENCE:\n",
    "        print(f\"Early stopping triggered: No improvement in Validation Loss for {stagnating_epochs} epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542f032",
   "metadata": {
    "papermill": {
     "duration": 0.005275,
     "end_time": "2025-05-05T07:37:22.541525",
     "exception": false,
     "start_time": "2025-05-05T07:37:22.536250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63114a0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:37:22.553350Z",
     "iopub.status.busy": "2025-05-05T07:37:22.552789Z",
     "iopub.status.idle": "2025-05-05T07:37:22.557256Z",
     "shell.execute_reply": "2025-05-05T07:37:22.556695Z"
    },
    "papermill": {
     "duration": 0.011468,
     "end_time": "2025-05-05T07:37:22.558214",
     "exception": false,
     "start_time": "2025-05-05T07:37:22.546746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reset randomness for non-deterministic output\n",
    "random.seed()\n",
    "np.random.seed(None)\n",
    "torch.backends.cudnn.deterministic = False #set both back to defaults\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e09c170e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:37:22.569521Z",
     "iopub.status.busy": "2025-05-05T07:37:22.569301Z",
     "iopub.status.idle": "2025-05-05T07:37:22.767797Z",
     "shell.execute_reply": "2025-05-05T07:37:22.766842Z"
    },
    "papermill": {
     "duration": 0.205375,
     "end_time": "2025-05-05T07:37:22.768890",
     "exception": false,
     "start_time": "2025-05-05T07:37:22.563515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/668548706.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(final_dir, \"best_model.pth\")) #load best model (in case early stopping or no valid loss improvement at the end)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluation\n",
    "checkpoint = torch.load(os.path.join(final_dir, \"best_model.pth\")) #load best model (in case early stopping or no valid loss improvement at the end)\n",
    "model.load_state_dict(checkpoint['model_state_dict']) #load model params into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58109bec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:37:22.781082Z",
     "iopub.status.busy": "2025-05-05T07:37:22.780857Z",
     "iopub.status.idle": "2025-05-05T07:37:22.826137Z",
     "shell.execute_reply": "2025-05-05T07:37:22.825616Z"
    },
    "papermill": {
     "duration": 0.05246,
     "end_time": "2025-05-05T07:37:22.827281",
     "exception": false,
     "start_time": "2025-05-05T07:37:22.774821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load test data\n",
    "test_df = pd.read_csv(\"./data/test_data.csv\")\n",
    "\n",
    "#wrap in dataset and use in DataLoader\n",
    "test_dataset = ThoughtDataset(test_df, src_vocab, tgt_vocab)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=pad_batch) #fix batch problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b850a62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:37:22.839247Z",
     "iopub.status.busy": "2025-05-05T07:37:22.839021Z",
     "iopub.status.idle": "2025-05-05T07:39:45.400495Z",
     "shell.execute_reply": "2025-05-05T07:39:45.399673Z"
    },
    "papermill": {
     "duration": 142.574696,
     "end_time": "2025-05-05T07:39:45.407561",
     "exception": false,
     "start_time": "2025-05-05T07:37:22.832865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time: 142.55 seconds\n"
     ]
    }
   ],
   "source": [
    "#init\n",
    "generated_texts = []\n",
    "true_texts = []\n",
    "original_texts = []\n",
    "indices = []\n",
    "\n",
    "test_start = time.time() #how long testing takes\n",
    "model.eval() #eval time\n",
    "with torch.no_grad(): #no gradients computed for predict\n",
    "    \n",
    "    for src, tgt, idx in test_loader:\n",
    "        #encode time (use final state for input for predict)\n",
    "        enc_outputs, hidden_state = model.encoder(src)\n",
    "        \n",
    "        #get target input for decoding - and batch size so first dim matches up\n",
    "        batch_size = src.size(0)\n",
    "        tgt_input = torch.tensor([tgt_vocab[sos_token]] * batch_size).to(device)\n",
    "        outputs = [] #init proper size\n",
    "        \n",
    "        for _ in range(MAX_LEN):\n",
    "            #decode time\n",
    "            output, hidden_state = model.decoder(tgt_input, hidden_state, enc_outputs)\n",
    "            output = output.squeeze(1) #not needed, but good just in case\n",
    "            \n",
    "            #sample token from predicted prob distribution (not just top token)\n",
    "            prob_dist = torch.softmax(output, dim=1)\n",
    "            pred_token = sample_next_token(prob_dist, temperature=TEMPERATURE)\n",
    "            \n",
    "            if pred_token.item() == tgt_vocab[eos_token]: #if end of sequence\n",
    "                break\n",
    "                \n",
    "            outputs.append(pred_token.item()) #add to output\n",
    "            tgt_input = pred_token #bc of squeeze, change tgt_input assignment\n",
    "\n",
    "        #decode into readable text - generated text = pred reframe\n",
    "        generated = \" \".join([inv_tgt_vocab.get(tok, '') for tok in outputs])\n",
    "        \n",
    "        #true text = dataset reframe\n",
    "        true = \" \".join([inv_tgt_vocab.get(tok.item(), '') for tok in tgt[0] if tok.item() not in [tgt_vocab[pad_token], tgt_vocab[sos_token], tgt_vocab[eos_token]]])\n",
    "        \n",
    "        #original text = negative thought (kept just in case tokenization/reconstruction changes it in meaningful way, good to keep a copy in case)\n",
    "        original = \" \".join([list(src_vocab.keys())[list(src_vocab.values()).index(tok.item())] for tok in src[0] if tok.item() != PAD_VAL])\n",
    "        \n",
    "        generated_texts.append(generated)\n",
    "        true_texts.append(true)\n",
    "        original_texts.append(original)\n",
    "        indices.append(idx)\n",
    "\n",
    "test_end = time.time()\n",
    "print(f\"Testing time: {test_end - test_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06989105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:39:45.419490Z",
     "iopub.status.busy": "2025-05-05T07:39:45.419240Z",
     "iopub.status.idle": "2025-05-05T07:39:45.524196Z",
     "shell.execute_reply": "2025-05-05T07:39:45.523598Z"
    },
    "papermill": {
     "duration": 0.112324,
     "end_time": "2025-05-05T07:39:45.525451",
     "exception": false,
     "start_time": "2025-05-05T07:39:45.413127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save to csv\n",
    "output_df = pd.DataFrame({\n",
    "    \"Original_Index\": indices,\n",
    "    \"Original_Text\": original_texts,\n",
    "    \"True_Text\": true_texts,\n",
    "    \"Generated_Text\": generated_texts\n",
    "})\n",
    "#sort by index to restore original csv order first\n",
    "output_df = output_df.sort_values(by=\"Original_Index\").reset_index(drop=True)\n",
    "output_df.drop(columns=[\"Original_Index\"], inplace=True) #not needed after sort\n",
    "output_df.to_csv(model_outputs_dir + \"/generated_output_seq2seq_non_deterministic.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7315971,
     "sourceId": 11658077,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2266.307326,
   "end_time": "2025-05-05T07:39:49.159414",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-05T07:02:02.852088",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
