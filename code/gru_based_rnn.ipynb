{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "958a8c47",
   "metadata": {
    "papermill": {
     "duration": 0.003939,
     "end_time": "2025-05-05T08:22:45.533704",
     "exception": false,
     "start_time": "2025-05-05T08:22:45.529765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0b6fff",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-05T08:22:45.540842Z",
     "iopub.status.busy": "2025-05-05T08:22:45.540634Z",
     "iopub.status.idle": "2025-05-05T08:22:52.656137Z",
     "shell.execute_reply": "2025-05-05T08:22:52.655314Z"
    },
    "papermill": {
     "duration": 7.120584,
     "end_time": "2025-05-05T08:22:52.657572",
     "exception": false,
     "start_time": "2025-05-05T08:22:45.536988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cd38f7",
   "metadata": {
    "papermill": {
     "duration": 0.003007,
     "end_time": "2025-05-05T08:22:52.664147",
     "exception": false,
     "start_time": "2025-05-05T08:22:52.661140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing/Model Preparations/Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec3628a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:22:52.671697Z",
     "iopub.status.busy": "2025-05-05T08:22:52.670978Z",
     "iopub.status.idle": "2025-05-05T08:22:52.812400Z",
     "shell.execute_reply": "2025-05-05T08:22:52.811493Z"
    },
    "papermill": {
     "duration": 0.146268,
     "end_time": "2025-05-05T08:22:52.813504",
     "exception": false,
     "start_time": "2025-05-05T08:22:52.667236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download Punkt tokenizer data - necessary for sentence tokenization, which word_tokenize needs\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47fb355",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:22:52.820854Z",
     "iopub.status.busy": "2025-05-05T08:22:52.820636Z",
     "iopub.status.idle": "2025-05-05T08:22:52.830515Z",
     "shell.execute_reply": "2025-05-05T08:22:52.829996Z"
    },
    "papermill": {
     "duration": 0.014711,
     "end_time": "2025-05-05T08:22:52.831510",
     "exception": false,
     "start_time": "2025-05-05T08:22:52.816799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define hyperparams\n",
    "BATCH_SIZE = 64 #good batch size for this task\n",
    "EMBED_SIZE = 128 #large enough for capturing word relationships but not too much to overfit (was overfitting with 256)\n",
    "HIDDEN_SIZE = 256 #same here\n",
    "EPOCHS = 40 #very high number of epochs to ensure model trains well, will implement early stopping to counteract\n",
    "PATIENCE = 3 #amount of epochs to wait for until early stop - good val, considering total epoch num\n",
    "MAX_LEN = 140 #fits all data in dataset without truncating\n",
    "PAD_VAL = 2 #for designating what id pad_token is (used in pad batch and cross entropy loss) #maybe make when vocab made\n",
    "DROPOUT_RATE = 0.6 #to avoid overfitting (increase slightly)\n",
    "LEARNING_RATE = 0.0005 #below initial Adam optimizer lr (model was overfitting, trying to fix)\n",
    "MAX_NORM = 1.0 #ensures gradients don't get too large\n",
    "TEMPERATURE=0.8 #controls randomness (higher = more random but less accurate/coherent - 0.8 is balanced choice)\n",
    "RANDOM = 213 #random state for reproducibility (I like using this num)\n",
    "\n",
    "#randomness\n",
    "random.seed(RANDOM)\n",
    "torch.manual_seed(RANDOM) #remove for non-deterministic\n",
    "np.random.seed(RANDOM)\n",
    "torch.backends.cudnn.deterministic = True #ensures GPU operations are deterministic (only for training)\n",
    "torch.backends.cudnn.benchmark = False #disables auto-tuning of algorithms (ensures more reproducibility)\n",
    "\n",
    "#init special tokens - no mask tokens needed\n",
    "sos_token = \"<s>\"\n",
    "eos_token = \"</s>\"\n",
    "pad_token = \"<pad>\"\n",
    "unk_token = \"<unk>\"\n",
    "mask_token = \"<mask>\" #used to differentiate between negative and positive thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a86b05c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:22:52.838770Z",
     "iopub.status.busy": "2025-05-05T08:22:52.838578Z",
     "iopub.status.idle": "2025-05-05T08:22:52.898808Z",
     "shell.execute_reply": "2025-05-05T08:22:52.898060Z"
    },
    "papermill": {
     "duration": 0.065035,
     "end_time": "2025-05-05T08:22:52.899852",
     "exception": false,
     "start_time": "2025-05-05T08:22:52.834817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#make directory to save final model\n",
    "final_dir = \"./final/gru\"\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "#make directory to save final data\n",
    "model_outputs_dir = \"./model_outputs\"\n",
    "os.makedirs(model_outputs_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee308d79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:22:52.907486Z",
     "iopub.status.busy": "2025-05-05T08:22:52.906952Z",
     "iopub.status.idle": "2025-05-05T08:22:52.910357Z",
     "shell.execute_reply": "2025-05-05T08:22:52.909667Z"
    },
    "papermill": {
     "duration": 0.008306,
     "end_time": "2025-05-05T08:22:52.911482",
     "exception": false,
     "start_time": "2025-05-05T08:22:52.903176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenize words w/ word_tokenize from nltk (handles punctuation & numbers properly (as separate tokens, good for natural text gen))\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text.lower()) #bc of above, only making lowercase is needed (contractions were removed, so won't be an issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dda69c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:22:52.918565Z",
     "iopub.status.busy": "2025-05-05T08:22:52.918367Z",
     "iopub.status.idle": "2025-05-05T08:22:52.922200Z",
     "shell.execute_reply": "2025-05-05T08:22:52.921538Z"
    },
    "papermill": {
     "duration": 0.008484,
     "end_time": "2025-05-05T08:22:52.923203",
     "exception": false,
     "start_time": "2025-05-05T08:22:52.914719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#build vocab with every word encountered so no loss in generation ability (unknown tokens would make it difficult)\n",
    "def build_vocab(sentences):\n",
    "    #init with special tokens\n",
    "    vocab = {sos_token: 0, eos_token: 1, pad_token: 2, unk_token: 3, mask_token: 4}\n",
    "    \n",
    "    #iterate through all tokenized sentences\n",
    "    for sent in sentences:\n",
    "        for word in tokenize(sent):\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab) #add to vocab with new id\n",
    "                \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "012eca35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:22:52.930176Z",
     "iopub.status.busy": "2025-05-05T08:22:52.929910Z",
     "iopub.status.idle": "2025-05-05T08:22:52.933256Z",
     "shell.execute_reply": "2025-05-05T08:22:52.932756Z"
    },
    "papermill": {
     "duration": 0.007862,
     "end_time": "2025-05-05T08:22:52.934255",
     "exception": false,
     "start_time": "2025-05-05T08:22:52.926393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#assign tokens to ids in sentences using vocab\n",
    "def token_to_ids(sentence, vocab):\n",
    "    #replace with <unk> if not in vocab, else id\n",
    "    return [vocab.get(word, vocab[unk_token]) for word in tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c8e5549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:22:52.941333Z",
     "iopub.status.busy": "2025-05-05T08:22:52.941118Z",
     "iopub.status.idle": "2025-05-05T08:22:52.946019Z",
     "shell.execute_reply": "2025-05-05T08:22:52.945366Z"
    },
    "papermill": {
     "duration": 0.009586,
     "end_time": "2025-05-05T08:22:52.946996",
     "exception": false,
     "start_time": "2025-05-05T08:22:52.937410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#put data in custom dataset so it works with DataLoader (won't work with anything without __len__ and __getitem__)\n",
    "class ReframedThoughtsDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        self.df = df.reset_index(drop=False)  #store original index\n",
    "        #get vocab and sentences\n",
    "        self.input_sentences = df[\"negative_thought\"]\n",
    "        self.target_sentences = df[\"reframed_thought\"]\n",
    "        self.vocab = vocab\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_sentences)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #get tokens (using mask token as separator but ensuring it doesn't get passed through tokenizer)\n",
    "        tokens = [self.vocab[sos_token]] + token_to_ids(self.input_sentences[idx], self.vocab) + [self.vocab[mask_token]] + token_to_ids(self.target_sentences[idx], self.vocab) + [self.vocab[eos_token]]\n",
    "        #convert to pytorch tensor\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        index = self.df.loc[idx, \"index\"] #get index\n",
    "        \n",
    "        #send to GPU\n",
    "        return tokens.to(device), index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff97966c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:22:52.953973Z",
     "iopub.status.busy": "2025-05-05T08:22:52.953782Z",
     "iopub.status.idle": "2025-05-05T08:22:52.957545Z",
     "shell.execute_reply": "2025-05-05T08:22:52.957031Z"
    },
    "papermill": {
     "duration": 0.008335,
     "end_time": "2025-05-05T08:22:52.958550",
     "exception": false,
     "start_time": "2025-05-05T08:22:52.950215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pad batches to make same size (use as collate function)\n",
    "def pad_batch(batch):\n",
    "    #separate tokens and indices in the batch (so they don't get mixed up)\n",
    "    tokens_batch = [item[0] for item in batch]  #list of tokenized sequences\n",
    "    indices_batch = [item[1] for item in batch]  #list of indices\n",
    "    \n",
    "    #use pad sequence - finds longest sequence in batch to automatically pad to\n",
    "    batch = pad_sequence(tokens_batch, padding_value=PAD_VAL, batch_first=True)\n",
    "    \n",
    "    #remove last token from input and first token from output (setting up next-token prediction)\n",
    "    inputs = batch[:, :-1]\n",
    "    targets = batch[:, 1:]\n",
    "    \n",
    "    return inputs.to(device), targets.to(device), indices_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33e2217f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:22:52.966124Z",
     "iopub.status.busy": "2025-05-05T08:22:52.965596Z",
     "iopub.status.idle": "2025-05-05T08:22:52.970110Z",
     "shell.execute_reply": "2025-05-05T08:22:52.969636Z"
    },
    "papermill": {
     "duration": 0.009251,
     "end_time": "2025-05-05T08:22:52.971085",
     "exception": false,
     "start_time": "2025-05-05T08:22:52.961834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#gru based rnn language model\n",
    "class GRULanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        #embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_VAL)\n",
    "        \n",
    "        #gru time - 2 layers to make more complex (prevent underfitting)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        \n",
    "        #dropout\n",
    "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
    "        \n",
    "        #fully connected output layer (map hidden state back to vocab for prediction) \n",
    "        self.out_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):  \n",
    "        #embed + dropout\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "\n",
    "        #gru time - don't need hidden state here\n",
    "        output, _ = self.gru(embedded)\n",
    "        \n",
    "        #get logits from output layer\n",
    "        logits = self.out_layer(output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a6ff2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:22:52.977947Z",
     "iopub.status.busy": "2025-05-05T08:22:52.977754Z",
     "iopub.status.idle": "2025-05-05T08:22:55.014971Z",
     "shell.execute_reply": "2025-05-05T08:22:55.014421Z"
    },
    "papermill": {
     "duration": 2.042005,
     "end_time": "2025-05-05T08:22:55.016273",
     "exception": false,
     "start_time": "2025-05-05T08:22:52.974268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load raw dataset\n",
    "train_df = pd.read_csv(\"./data/train_data.csv\")\n",
    "valid_df = pd.read_csv(\"./data/valid_data.csv\")\n",
    "\n",
    "#build vocab off of training data - including inverse_vocab for target (for making readable at end)\n",
    "vocab = build_vocab(train_df[\"negative_thought\"])\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f813ad69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:22:55.023896Z",
     "iopub.status.busy": "2025-05-05T08:22:55.023686Z",
     "iopub.status.idle": "2025-05-05T08:22:55.030140Z",
     "shell.execute_reply": "2025-05-05T08:22:55.029516Z"
    },
    "papermill": {
     "duration": 0.011504,
     "end_time": "2025-05-05T08:22:55.031301",
     "exception": false,
     "start_time": "2025-05-05T08:22:55.019797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#wrap in dataset\n",
    "train_dataset = ReframedThoughtsDataset(train_df, vocab)\n",
    "valid_dataset = ReframedThoughtsDataset(valid_df, vocab)\n",
    "\n",
    "#use DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1043e7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:22:55.038547Z",
     "iopub.status.busy": "2025-05-05T08:22:55.038133Z",
     "iopub.status.idle": "2025-05-05T08:22:59.479493Z",
     "shell.execute_reply": "2025-05-05T08:22:59.478879Z"
    },
    "papermill": {
     "duration": 4.446336,
     "end_time": "2025-05-05T08:22:59.480816",
     "exception": false,
     "start_time": "2025-05-05T08:22:55.034480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#init language model\n",
    "model = GRULanguageModel(len(vocab), EMBED_SIZE, HIDDEN_SIZE).to(device)\n",
    "\n",
    "#get optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE) #adam optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_VAL) #ignore padding tokens for loss calc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc355f",
   "metadata": {
    "papermill": {
     "duration": 0.003121,
     "end_time": "2025-05-05T08:22:59.487464",
     "exception": false,
     "start_time": "2025-05-05T08:22:59.484343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f02c1789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:22:59.495322Z",
     "iopub.status.busy": "2025-05-05T08:22:59.495012Z",
     "iopub.status.idle": "2025-05-05T08:32:54.819892Z",
     "shell.execute_reply": "2025-05-05T08:32:54.819079Z"
    },
    "papermill": {
     "duration": 595.335993,
     "end_time": "2025-05-05T08:32:54.826616",
     "exception": false,
     "start_time": "2025-05-05T08:22:59.490623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 5.6326\n",
      "Epoch 1, Validation Loss: 4.9868\n",
      "Epoch 1 summary:\n",
      "  Training time: 13.86 seconds\n",
      "  Validation time: 2.42 seconds\n",
      "  Total epoch time: 16.28 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 2, Training Loss: 4.7929\n",
      "Epoch 2, Validation Loss: 4.4541\n",
      "Epoch 2 summary:\n",
      "  Training time: 13.12 seconds\n",
      "  Validation time: 2.48 seconds\n",
      "  Total epoch time: 15.61 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 3, Training Loss: 4.4185\n",
      "Epoch 3, Validation Loss: 4.2307\n",
      "Epoch 3 summary:\n",
      "  Training time: 13.19 seconds\n",
      "  Validation time: 2.40 seconds\n",
      "  Total epoch time: 15.59 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 4, Training Loss: 4.2234\n",
      "Epoch 4, Validation Loss: 4.0928\n",
      "Epoch 4 summary:\n",
      "  Training time: 13.16 seconds\n",
      "  Validation time: 2.46 seconds\n",
      "  Total epoch time: 15.62 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 5, Training Loss: 4.0919\n",
      "Epoch 5, Validation Loss: 4.0161\n",
      "Epoch 5 summary:\n",
      "  Training time: 13.08 seconds\n",
      "  Validation time: 2.43 seconds\n",
      "  Total epoch time: 15.52 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 6, Training Loss: 3.9928\n",
      "Epoch 6, Validation Loss: 3.9512\n",
      "Epoch 6 summary:\n",
      "  Training time: 13.06 seconds\n",
      "  Validation time: 2.47 seconds\n",
      "  Total epoch time: 15.53 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 7, Training Loss: 3.9105\n",
      "Epoch 7, Validation Loss: 3.8943\n",
      "Epoch 7 summary:\n",
      "  Training time: 13.18 seconds\n",
      "  Validation time: 2.42 seconds\n",
      "  Total epoch time: 15.60 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 8, Training Loss: 3.8396\n",
      "Epoch 8, Validation Loss: 3.8630\n",
      "Epoch 8 summary:\n",
      "  Training time: 13.14 seconds\n",
      "  Validation time: 2.46 seconds\n",
      "  Total epoch time: 15.60 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 9, Training Loss: 3.7746\n",
      "Epoch 9, Validation Loss: 3.8128\n",
      "Epoch 9 summary:\n",
      "  Training time: 13.17 seconds\n",
      "  Validation time: 2.40 seconds\n",
      "  Total epoch time: 15.57 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 10, Training Loss: 3.7118\n",
      "Epoch 10, Validation Loss: 3.7772\n",
      "Epoch 10 summary:\n",
      "  Training time: 13.09 seconds\n",
      "  Validation time: 2.48 seconds\n",
      "  Total epoch time: 15.57 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 11, Training Loss: 3.6535\n",
      "Epoch 11, Validation Loss: 3.7423\n",
      "Epoch 11 summary:\n",
      "  Training time: 13.21 seconds\n",
      "  Validation time: 2.39 seconds\n",
      "  Total epoch time: 15.61 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 12, Training Loss: 3.5993\n",
      "Epoch 12, Validation Loss: 3.7196\n",
      "Epoch 12 summary:\n",
      "  Training time: 13.17 seconds\n",
      "  Validation time: 2.52 seconds\n",
      "  Total epoch time: 15.69 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 13, Training Loss: 3.5478\n",
      "Epoch 13, Validation Loss: 3.6868\n",
      "Epoch 13 summary:\n",
      "  Training time: 13.14 seconds\n",
      "  Validation time: 2.39 seconds\n",
      "  Total epoch time: 15.53 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 14, Training Loss: 3.5002\n",
      "Epoch 14, Validation Loss: 3.6691\n",
      "Epoch 14 summary:\n",
      "  Training time: 13.12 seconds\n",
      "  Validation time: 2.50 seconds\n",
      "  Total epoch time: 15.62 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 15, Training Loss: 3.4516\n",
      "Epoch 15, Validation Loss: 3.6459\n",
      "Epoch 15 summary:\n",
      "  Training time: 13.18 seconds\n",
      "  Validation time: 2.39 seconds\n",
      "  Total epoch time: 15.57 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 16, Training Loss: 3.4065\n",
      "Epoch 16, Validation Loss: 3.6320\n",
      "Epoch 16 summary:\n",
      "  Training time: 13.20 seconds\n",
      "  Validation time: 2.49 seconds\n",
      "  Total epoch time: 15.69 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 17, Training Loss: 3.3631\n",
      "Epoch 17, Validation Loss: 3.6253\n",
      "Epoch 17 summary:\n",
      "  Training time: 13.08 seconds\n",
      "  Validation time: 2.40 seconds\n",
      "  Total epoch time: 15.49 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 18, Training Loss: 3.3209\n",
      "Epoch 18, Validation Loss: 3.6042\n",
      "Epoch 18 summary:\n",
      "  Training time: 13.14 seconds\n",
      "  Validation time: 2.46 seconds\n",
      "  Total epoch time: 15.61 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 19, Training Loss: 3.2808\n",
      "Epoch 19, Validation Loss: 3.5927\n",
      "Epoch 19 summary:\n",
      "  Training time: 13.18 seconds\n",
      "  Validation time: 2.43 seconds\n",
      "  Total epoch time: 15.62 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 20, Training Loss: 3.2418\n",
      "Epoch 20, Validation Loss: 3.5796\n",
      "Epoch 20 summary:\n",
      "  Training time: 13.15 seconds\n",
      "  Validation time: 2.47 seconds\n",
      "  Total epoch time: 15.62 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 21, Training Loss: 3.2019\n",
      "Epoch 21, Validation Loss: 3.5725\n",
      "Epoch 21 summary:\n",
      "  Training time: 13.09 seconds\n",
      "  Validation time: 2.43 seconds\n",
      "  Total epoch time: 15.52 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 22, Training Loss: 3.1622\n",
      "Epoch 22, Validation Loss: 3.5629\n",
      "Epoch 22 summary:\n",
      "  Training time: 13.12 seconds\n",
      "  Validation time: 2.50 seconds\n",
      "  Total epoch time: 15.62 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 23, Training Loss: 3.1264\n",
      "Epoch 23, Validation Loss: 3.5554\n",
      "Epoch 23 summary:\n",
      "  Training time: 13.18 seconds\n",
      "  Validation time: 2.39 seconds\n",
      "  Total epoch time: 15.57 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 24, Training Loss: 3.0924\n",
      "Epoch 24, Validation Loss: 3.5527\n",
      "Epoch 24 summary:\n",
      "  Training time: 13.13 seconds\n",
      "  Validation time: 2.48 seconds\n",
      "  Total epoch time: 15.61 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 25, Training Loss: 3.0594\n",
      "Epoch 25, Validation Loss: 3.5437\n",
      "Epoch 25 summary:\n",
      "  Training time: 13.14 seconds\n",
      "  Validation time: 2.40 seconds\n",
      "  Total epoch time: 15.54 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 26, Training Loss: 3.0285\n",
      "Epoch 26, Validation Loss: 3.5396\n",
      "Epoch 26 summary:\n",
      "  Training time: 13.14 seconds\n",
      "  Validation time: 2.46 seconds\n",
      "  Total epoch time: 15.60 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 27, Training Loss: 2.9988\n",
      "Epoch 27, Validation Loss: 3.5363\n",
      "Epoch 27 summary:\n",
      "  Training time: 13.21 seconds\n",
      "  Validation time: 2.40 seconds\n",
      "  Total epoch time: 15.61 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 28, Training Loss: 2.9709\n",
      "Epoch 28, Validation Loss: 3.5368\n",
      "Epoch 28 summary:\n",
      "  Training time: 13.17 seconds\n",
      "  Validation time: 2.50 seconds\n",
      "  Total epoch time: 15.66 seconds\n",
      "\n",
      "No Validation Loss Improvement. Num of epochs with no improvement: 1/3.\n",
      "Epoch 29, Training Loss: 2.9424\n",
      "Epoch 29, Validation Loss: 3.5334\n",
      "Epoch 29 summary:\n",
      "  Training time: 13.17 seconds\n",
      "  Validation time: 2.40 seconds\n",
      "  Total epoch time: 15.57 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 30, Training Loss: 2.9175\n",
      "Epoch 30, Validation Loss: 3.5318\n",
      "Epoch 30 summary:\n",
      "  Training time: 13.14 seconds\n",
      "  Validation time: 2.45 seconds\n",
      "  Total epoch time: 15.59 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 31, Training Loss: 2.8907\n",
      "Epoch 31, Validation Loss: 3.5318\n",
      "Epoch 31 summary:\n",
      "  Training time: 13.19 seconds\n",
      "  Validation time: 2.39 seconds\n",
      "  Total epoch time: 15.58 seconds\n",
      "\n",
      "No Validation Loss Improvement. Num of epochs with no improvement: 1/3.\n",
      "Epoch 32, Training Loss: 2.8665\n",
      "Epoch 32, Validation Loss: 3.5298\n",
      "Epoch 32 summary:\n",
      "  Training time: 13.17 seconds\n",
      "  Validation time: 2.39 seconds\n",
      "  Total epoch time: 15.56 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 33, Training Loss: 2.8460\n",
      "Epoch 33, Validation Loss: 3.5317\n",
      "Epoch 33 summary:\n",
      "  Training time: 13.14 seconds\n",
      "  Validation time: 2.42 seconds\n",
      "  Total epoch time: 15.56 seconds\n",
      "\n",
      "No Validation Loss Improvement. Num of epochs with no improvement: 1/3.\n",
      "Epoch 34, Training Loss: 2.8226\n",
      "Epoch 34, Validation Loss: 3.5332\n",
      "Epoch 34 summary:\n",
      "  Training time: 13.15 seconds\n",
      "  Validation time: 2.39 seconds\n",
      "  Total epoch time: 15.54 seconds\n",
      "\n",
      "No Validation Loss Improvement. Num of epochs with no improvement: 2/3.\n",
      "Epoch 35, Training Loss: 2.8012\n",
      "Epoch 35, Validation Loss: 3.5294\n",
      "Epoch 35 summary:\n",
      "  Training time: 13.22 seconds\n",
      "  Validation time: 2.44 seconds\n",
      "  Total epoch time: 15.66 seconds\n",
      "\n",
      "Validation Loss improved!\n",
      "Epoch 36, Training Loss: 2.7796\n",
      "Epoch 36, Validation Loss: 3.5320\n",
      "Epoch 36 summary:\n",
      "  Training time: 13.22 seconds\n",
      "  Validation time: 2.42 seconds\n",
      "  Total epoch time: 15.63 seconds\n",
      "\n",
      "No Validation Loss Improvement. Num of epochs with no improvement: 1/3.\n",
      "Epoch 37, Training Loss: 2.7582\n",
      "Epoch 37, Validation Loss: 3.5343\n",
      "Epoch 37 summary:\n",
      "  Training time: 13.15 seconds\n",
      "  Validation time: 2.43 seconds\n",
      "  Total epoch time: 15.58 seconds\n",
      "\n",
      "No Validation Loss Improvement. Num of epochs with no improvement: 2/3.\n",
      "Epoch 38, Training Loss: 2.7393\n",
      "Epoch 38, Validation Loss: 3.5357\n",
      "Epoch 38 summary:\n",
      "  Training time: 13.14 seconds\n",
      "  Validation time: 2.40 seconds\n",
      "  Total epoch time: 15.54 seconds\n",
      "\n",
      "No Validation Loss Improvement. Num of epochs with no improvement: 3/3.\n",
      "Early stopping triggered: No improvement in Validation Loss for 3 epochs.\n"
     ]
    }
   ],
   "source": [
    "#for early stop\n",
    "best_valid_loss = float(\"inf\") #init with high valid loss\n",
    "stagnating_epochs = 0 #epoch num with no improvement\n",
    "\n",
    "#training\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time() #how long whole epoch takes\n",
    "    train_start = time.time() #how long training takes\n",
    "    model.train() #training mode\n",
    "    train_loss = 0 #init loss\n",
    "    \n",
    "    for input_token, next_token, _ in train_loader:\n",
    "        optimizer.zero_grad() #clear optimized tensor gradients that accumulate in backward pass\n",
    "\n",
    "        #ensure input token and next token are on same device as the model\n",
    "        input_token = input_token.to(device)\n",
    "        next_token = next_token.to(device)\n",
    "        \n",
    "        #get output token\n",
    "        logits = model(input_token)\n",
    "        logits_dim = logits.shape[-1]\n",
    "        logits = logits.reshape(-1, logits_dim)\n",
    "        \n",
    "        #get true next token\n",
    "        next_token = next_token.reshape(-1)\n",
    "        \n",
    "        loss = criterion(logits, next_token) #compare (loss time)\n",
    "        loss.backward() #backpropagation!!!\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=MAX_NORM) #gradient clipping (to be safe with a weaker model)\n",
    "        \n",
    "        optimizer.step() #update weights\n",
    "        train_loss += loss.item() #add to loss\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss/len(train_loader):.4f}\")\n",
    "    train_end = time.time()\n",
    "\n",
    "    #validation loop fr\n",
    "    valid_start = time.time() #how long validation takes\n",
    "    model.eval() #bc we're predicting rn\n",
    "    valid_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_token, next_token, _ in valid_loader:\n",
    "            #same thing but no backprop/updating weights\n",
    "            input_token = input_token.to(device)\n",
    "            next_token = next_token.to(device)\n",
    "            \n",
    "            logits = model(input_token)\n",
    "            logits_dim = logits.shape[-1]\n",
    "            logits = logits.reshape(-1, logits_dim)\n",
    "            \n",
    "            next_token = next_token.reshape(-1)\n",
    "            \n",
    "            loss = criterion(logits, next_token)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {valid_loss/len(valid_loader):.4f}\")\n",
    "    valid_end = time.time()\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} summary:\")\n",
    "    print(f\"  Training time: {train_end - train_start:.2f} seconds\")\n",
    "    print(f\"  Validation time: {valid_end - valid_start:.2f} seconds\")\n",
    "    print(f\"  Total epoch time: {epoch_end - epoch_start:.2f} seconds\\n\")\n",
    "\n",
    "    #early stop check! (if better valid loss, update. otherwise, update stagnating epochs) \n",
    "    if valid_loss/len(valid_loader) < best_valid_loss:\n",
    "        best_valid_loss = valid_loss/len(valid_loader) \n",
    "        stagnating_epochs = 0 #reset\n",
    "        print(f\"Validation Loss improved!\")\n",
    "        #save model with best validation loss (in case of early stop)\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": loss.item()\n",
    "        }, os.path.join(final_dir, \"best_model.pth\"))\n",
    "    else:\n",
    "        stagnating_epochs += 1 #increase\n",
    "        print(f\"No Validation Loss Improvement. Num of epochs with no improvement: {stagnating_epochs}/{PATIENCE}.\")\n",
    "\n",
    "    #if patience reached, end\n",
    "    if stagnating_epochs >= PATIENCE:\n",
    "        print(f\"Early stopping triggered: No improvement in Validation Loss for {stagnating_epochs} epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b11b61d",
   "metadata": {
    "papermill": {
     "duration": 0.005752,
     "end_time": "2025-05-05T08:32:54.838401",
     "exception": false,
     "start_time": "2025-05-05T08:32:54.832649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "934e8bf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:32:54.851451Z",
     "iopub.status.busy": "2025-05-05T08:32:54.851013Z",
     "iopub.status.idle": "2025-05-05T08:32:54.885688Z",
     "shell.execute_reply": "2025-05-05T08:32:54.884988Z"
    },
    "papermill": {
     "duration": 0.042512,
     "end_time": "2025-05-05T08:32:54.886806",
     "exception": false,
     "start_time": "2025-05-05T08:32:54.844294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/668548706.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(final_dir, \"best_model.pth\")) #load best model (in case early stopping or no valid loss improvement at the end)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluation\n",
    "checkpoint = torch.load(os.path.join(final_dir, \"best_model.pth\")) #load best model (in case early stopping or no valid loss improvement at the end)\n",
    "model.load_state_dict(checkpoint['model_state_dict']) #load model params into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "539766d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:32:54.900207Z",
     "iopub.status.busy": "2025-05-05T08:32:54.899981Z",
     "iopub.status.idle": "2025-05-05T08:32:54.934901Z",
     "shell.execute_reply": "2025-05-05T08:32:54.934423Z"
    },
    "papermill": {
     "duration": 0.042623,
     "end_time": "2025-05-05T08:32:54.935882",
     "exception": false,
     "start_time": "2025-05-05T08:32:54.893259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load test data\n",
    "test_df = pd.read_csv(\"./data/test_data.csv\")\n",
    "\n",
    "#wrap in dataset and use in DataLoader\n",
    "test_dataset = ReframedThoughtsDataset(test_df, vocab)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=pad_batch) #for testing all at once (without doing the batch predict thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81d7aff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:32:54.949211Z",
     "iopub.status.busy": "2025-05-05T08:32:54.949021Z",
     "iopub.status.idle": "2025-05-05T08:33:48.462353Z",
     "shell.execute_reply": "2025-05-05T08:33:48.461556Z"
    },
    "papermill": {
     "duration": 53.527521,
     "end_time": "2025-05-05T08:33:48.469653",
     "exception": false,
     "start_time": "2025-05-05T08:32:54.942132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time: 53.51 seconds\n"
     ]
    }
   ],
   "source": [
    "#init\n",
    "generated_texts = []\n",
    "true_texts = []\n",
    "original_texts = []\n",
    "indices = []\n",
    "\n",
    "test_start = time.time() #how long testing takes\n",
    "model.eval() #eval time\n",
    "with torch.no_grad(): #no gradients computed for predict\n",
    "    for idx in range(len(test_dataset)): #iterates over whole dataset (since batch size is 1 and test dataset needs to be iterated over)\n",
    "        input_seq, idx = test_dataset[idx] #get original test input & index\n",
    "        input_token = torch.tensor([vocab[sos_token]] + input_seq.tolist(), device=device).unsqueeze(0) #safer way of making pytorch tensor and sending to GPU\n",
    "        generated = []\n",
    "        \n",
    "        for _ in range(MAX_LEN):\n",
    "            logits = model(input_token) #get logits\n",
    "            next_token = logits[:, -1].argmax(-1).item() #get next token from logits\n",
    "\n",
    "            if next_token == vocab[mask_token]: #if mask token has been hit\n",
    "                if next_token in generated: #if mask token is already in generated (don't want multiple, only the one separating negative and positive thoughts)\n",
    "                    #clone logits and make mask token logits very low (will pick 2nd most prob token instead)\n",
    "                    logits_no_mask = logits.clone()\n",
    "                    logits_no_mask[:, -1, vocab[mask_token]] = float(\"-inf\")\n",
    "                    next_token = logits_no_mask[:, -1].argmax(-1).item()\n",
    "                    \n",
    "                else: #once mask is hit, stop adding to input token\n",
    "                    input_token = torch.cat((input_token, torch.tensor([[next_token]], device=device)), dim=1)\n",
    "                    continue\n",
    "                \n",
    "            if next_token == vocab[eos_token]: #check if end of sequence\n",
    "                    break\n",
    "    \n",
    "            generated.append(next_token)\n",
    "    \n",
    "            input_token = torch.cat((input_token, torch.tensor([[next_token]], device=device)), dim=1) #add next token to input\n",
    "    \n",
    "        #decode into readable text - generated and original/true text\n",
    "        generated_text = \" \".join([inverse_vocab[t] for t in generated])\n",
    "        generated_texts.append(generated_text)\n",
    "        \n",
    "        all_true_text = \" \".join([inverse_vocab.get(tok.item(), '') for tok in input_seq if tok.item() not in [vocab[pad_token], vocab[sos_token], vocab[eos_token]]])\n",
    "        if len(all_true_text.split(mask_token)) == 2: #ensuring only one mask token per sequence (shouldn't be necessary, but defensive programming tactic)\n",
    "            #get original text (negative thought) and true text (reframed thought) separated for easier comparison\n",
    "            original_text, true_text = all_true_text.split(mask_token)\n",
    "            true_texts.append(true_text)\n",
    "            original_texts.append(original_text)\n",
    "\n",
    "        indices.append(idx) #store index\n",
    "            \n",
    "\n",
    "test_end = time.time()\n",
    "print(f\"Testing time: {test_end - test_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62526658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T08:33:48.482844Z",
     "iopub.status.busy": "2025-05-05T08:33:48.482643Z",
     "iopub.status.idle": "2025-05-05T08:33:48.516994Z",
     "shell.execute_reply": "2025-05-05T08:33:48.516392Z"
    },
    "papermill": {
     "duration": 0.042112,
     "end_time": "2025-05-05T08:33:48.518040",
     "exception": false,
     "start_time": "2025-05-05T08:33:48.475928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save to csv\n",
    "output_df = pd.DataFrame({\n",
    "    \"Original_Index\": indices,\n",
    "    \"Original_Text\": original_texts,\n",
    "    \"True_Text\": true_texts,\n",
    "    \"Generated_Text\": generated_texts\n",
    "})\n",
    "#sort by index to restore original csv order first\n",
    "output_df = output_df.sort_values(by=\"Original_Index\").reset_index(drop=True)\n",
    "output_df.drop(columns=[\"Original_Index\"], inplace=True) #not needed after sort\n",
    "output_df.to_csv(model_outputs_dir + \"/generated_output_gru.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7315971,
     "sourceId": 11658077,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 669.423362,
   "end_time": "2025-05-05T08:33:51.025808",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-05T08:22:41.602446",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
